# Configuration file for Hybrid Text Summarization

# Model configurations
models:
  extractive:
    textrank:
      damping: 0.85
      max_iter: 100
      tol: 1e-6
    
    bert_extractive:
      model_name: "sentence-transformers/all-MiniLM-L6-v2"
      similarity_threshold: 0.7
      max_sentences: 5
    
    lexrank:
      threshold: 0.1
      max_sentences: 5

  abstractive:
    t5:
      model_name: "t5-small"
      max_length: 512
      min_length: 50
      num_beams: 4
      early_stopping: true
    
    bart:
      model_name: "facebook/bart-large-cnn"
      max_length: 1024
      min_length: 100
      num_beams: 4
      early_stopping: true
    
    pegasus:
      model_name: "google/pegasus-xsum"
      max_length: 512
      min_length: 50
      num_beams: 4
      early_stopping: true

# Hybrid configuration
hybrid:
  methods:
    - "weighted_combination"
    - "pipeline"
    - "ensemble"
  
  weights:
    extractive: 0.4
    abstractive: 0.6
  
  combination_strategy: "weighted_combination"

# Evaluation metrics
evaluation:
  metrics:
    - "rouge-1"
    - "rouge-2"
    - "rouge-l"
    - "bleu"
    - "bert_score"
  
  rouge:
    use_stemmer: true
    rouge_l_max: 4
  
  bert_score:
    model_type: "microsoft/DialoGPT-medium"
    lang: "en"

# Data processing
data:
  max_input_length: 2048
  min_sentence_length: 10
  max_sentence_length: 200
  remove_duplicates: true
  language: "en"

# Training (if applicable)
training:
  batch_size: 8
  learning_rate: 5e-5
  num_epochs: 3
  warmup_steps: 500
  max_grad_norm: 1.0
  save_steps: 1000
  eval_steps: 500

# Logging
logging:
  level: "INFO"
  log_file: "logs/summarization.log"
  use_wandb: false
  wandb_project: "hybrid-text-summarization"

# Output
output:
  save_summaries: true
  output_dir: "outputs"
  format: "json"  # json, txt, html